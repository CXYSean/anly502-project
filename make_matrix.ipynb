{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "     .appName(\"Test SparkSession\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1990 = spark.read.csv(\"s3://chenxiyuanly502/project/2gram_1990\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2000 = spark.read.csv(\"s3://chenxiyuanly502/project/2gram_2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+----+----+\n",
      "|         _c0| _c1| _c2| _c3|\n",
      "+------------+----+----+----+\n",
      "|\"Bulverhithe| 4.0| 2.0| 4.0|\n",
      "|      Bur at|23.0|19.0|23.0|\n",
      "|    But Posy| 2.0| 2.0| 2.0|\n",
      "| Butler didn|60.0|50.0|60.0|\n",
      "|COUNCIL HELD|14.0| 9.0|13.0|\n",
      "+------------+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1990.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1990.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldColumns = df1990.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newColumns = [\"ngram\", \"occurrences\",\"books\",\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(oldColumns)):\n",
    "    df1990 = df1990.withColumnRenamed(oldColumns[i], newColumns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ngram: string (nullable = true)\n",
      " |-- occurrences: string (nullable = true)\n",
      " |-- books: string (nullable = true)\n",
      " |-- pages: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1990.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1900 = spark.read.csv(\"s3://chenxiyuanly502/project/2gram_1900\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1900.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(oldColumns)):\n",
    "    df1900 = df1900.withColumnRenamed(oldColumns[i], newColumns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(oldColumns)):\n",
    "    df2000 = df2000.withColumnRenamed(oldColumns[i], newColumns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+-----+\n",
      "|       ngram|occurrences|books|pages|\n",
      "+------------+-----------+-----+-----+\n",
      "|  \"Budweiser|        3.0|  2.0|  3.0|\n",
      "|  But doomed|       13.0| 13.0| 13.0|\n",
      "|  By Hasselt|       13.0| 13.0| 13.0|\n",
      "| Cadmus were|       10.0| 10.0| 10.0|\n",
      "|Caldigate as|       26.0|  5.0| 26.0|\n",
      "+------------+-----------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1900.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2000 = df2000.filter(df2000.occurrences >= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1505674"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2000 = df2000.filter(df2000.occurrences >= 1000)\n",
    "df2000.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943248"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1990 = df1990.filter(df1990.occurrences >= 1000)\n",
    "df1990.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldColumns = df2000.schema.names\n",
    "newColumns = [\"ngram\", \"occurrences2000\",\"books2000\",\"pages2000\"]\n",
    "for i in range(len(oldColumns)):\n",
    "    df2000 = df2000.withColumnRenamed(oldColumns[i], newColumns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldColumns = df1990.schema.names\n",
    "newColumns = [\"ngram\", \"occurrences1990\",\"books1990\",\"pages1990\"]\n",
    "for i in range(len(oldColumns)):\n",
    "    df1990 = df1990.withColumnRenamed(oldColumns[i], newColumns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2000.join(df1990, ['ngram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ngram: string (nullable = true)\n",
      " |-- occurrences2000: string (nullable = true)\n",
      " |-- books2000: string (nullable = true)\n",
      " |-- pages2000: string (nullable = true)\n",
      " |-- occurrences1990: string (nullable = true)\n",
      " |-- books1990: string (nullable = true)\n",
      " |-- pages1990: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+---------+---------+---------------+---------+---------+\n",
      "|         ngram|occurrences2000|books2000|pages2000|occurrences1990|books1990|pages1990|\n",
      "+--------------+---------------+---------+---------+---------------+---------+---------+\n",
      "|         ! All|        95973.0|  54183.0|  93545.0|        33508.0|  19552.0|  32669.0|\n",
      "|    \"\"\" After\"|       645983.0| 147470.0| 630108.0|       254774.0|  60127.0| 248130.0|\n",
      "|\"\"\" Afterward\"|         6122.0|   5115.0|   6050.0|         3319.0|   2711.0|   3280.0|\n",
      "| \"\"\" Bartleby\"|         1847.0|    283.0|   1370.0|         1345.0|    255.0|    942.0|\n",
      "|   \"\"\" Boomer\"|         3186.0|    355.0|   1824.0|         1958.0|    164.0|    910.0|\n",
      "+--------------+---------------+---------+---------+---------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df = df.sort(desc(\"occurrences2000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"occurrences2000\",  df['occurrences2000'].cast('int'))\n",
    "df = df.withColumn(\"books2000\",  df['books2000'].cast('int'))\n",
    "df = df.withColumn(\"pages2000\",  df['pages2000'].cast('int'))\n",
    "df = df.withColumn(\"occurrences1990\",  df['occurrences1990'].cast('int'))\n",
    "df = df.withColumn(\"books1990\",  df['books1990'].cast('int'))\n",
    "df = df.withColumn(\"pages1990\",  df['pages1990'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort(desc(\"occurrences2000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+---------+---------+---------------+---------+---------+\n",
      "|  ngram|occurrences2000|books2000|pages2000|occurrences1990|books1990|pages1990|\n",
      "+-------+---------------+---------+---------+---------------+---------+---------+\n",
      "|   Mr .|        9822203|   170081|  4952729|        2585207|    72938|  1349166|\n",
      "|she was|        9787582|   211182|  7116190|        3979782|    90275|  2850958|\n",
      "|   I am|        9732835|   228986|  6808620|        3127362|   104319|  2208289|\n",
      "|did not|        9679205|   230301|  7693735|        4344349|   109963|  3424445|\n",
      "|  to do|        9666537|   243301|  8124698|        4117244|   114076|  3448710|\n",
      "+-------+---------------+---------+---------+---------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Scaling :\n",
      "+-------+---------------+---------+---------+---------------+---------+---------+----------------------+\n",
      "|  ngram|occurrences2000|books2000|pages2000|occurrences1990|books1990|pages1990|occurrences1990_Scaled|\n",
      "+-------+---------------+---------+---------+---------------+---------+---------+----------------------+\n",
      "|   Mr .|        9822203|   170081|  4952729|        2585207|    72938|  1349166|                 0.362|\n",
      "|she was|        9787582|   211182|  7116190|        3979782|    90275|  2850958|                 0.557|\n",
      "|   I am|        9732835|   228986|  6808620|        3127362|   104319|  2208289|                 0.438|\n",
      "|did not|        9679205|   230301|  7693735|        4344349|   109963|  3424445|                 0.609|\n",
      "|  to do|        9666537|   243301|  8124698|        4117244|   114076|  3448710|                 0.577|\n",
      "+-------+---------------+---------+---------+---------------+---------+---------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# UDF for converting column type from vector to double type\n",
    "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "\n",
    "# Iterating over columns to be scaled\n",
    "for i in [\"occurrences2000\",\"occurrences1990\"]:\n",
    "    # VectorAssembler Transformation - Converting column to vector type\n",
    "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "    # MinMaxScaler Transformation\n",
    "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "    # Pipeline of VectorAssembler and MinMaxScaler\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    # Fitting pipeline on dataframe\n",
    "    l1df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "\n",
    "print(\"After Scaling :\")\n",
    "l1df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ngram: string (nullable = true)\n",
      " |-- occurrences2000: integer (nullable = true)\n",
      " |-- books2000: integer (nullable = true)\n",
      " |-- pages2000: integer (nullable = true)\n",
      " |-- occurrences1990: integer (nullable = true)\n",
      " |-- books1990: integer (nullable = true)\n",
      " |-- pages1990: integer (nullable = true)\n",
      " |-- occurrences1990_Scaled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ngram: string (nullable = true)\n",
      " |-- occurrences2000: integer (nullable = true)\n",
      " |-- occurrences1990: integer (nullable = true)\n",
      " |-- occurrences1990_Scaled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1df= l1df.drop('books2000')\n",
    "l1df= l1df.drop('pages2000')\n",
    "l1df= l1df.drop('books1990')\n",
    "l1df= l1df.drop('pages1990')\n",
    "l1df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Scaling :\n",
      "+-------+---------------+---------------+----------------------+----------------------+\n",
      "|  ngram|occurrences2000|occurrences1990|occurrences1990_Scaled|occurrences2000_Scaled|\n",
      "+-------+---------------+---------------+----------------------+----------------------+\n",
      "|   Mr .|        9822203|        2585207|                 0.362|                   1.0|\n",
      "|she was|        9787582|        3979782|                 0.557|                 0.996|\n",
      "|   I am|        9732835|        3127362|                 0.438|                 0.991|\n",
      "|did not|        9679205|        4344349|                 0.609|                 0.985|\n",
      "|  to do|        9666537|        4117244|                 0.577|                 0.984|\n",
      "+-------+---------------+---------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = \"occurrences2000\"\n",
    "assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "# MinMaxScaler Transformation\n",
    "scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "# Pipeline of VectorAssembler and MinMaxScaler\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fitting pipeline on dataframe\n",
    "l1df = pipeline.fit(l1df).transform(l1df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "\n",
    "print(\"After Scaling :\")\n",
    "l1df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Scaling :\n",
      "+--------------+---------------+---------------+----------------------+----------------------+---------------+----------------------+\n",
      "|         ngram|occurrences2000|occurrences1990|occurrences1990_Scaled|occurrences2000_Scaled|occurrences1970|occurrences1970_Scaled|\n",
      "+--------------+---------------+---------------+----------------------+----------------------+---------------+----------------------+\n",
      "|         ! All|          95973|          33508|                 0.005|                  0.01|          14648|                 0.004|\n",
      "|    \"\"\" After\"|         645983|         254774|                 0.036|                 0.066|          78929|                 0.021|\n",
      "|\"\"\" Afterward\"|           6122|           3319|                   0.0|                 0.001|            836|                   0.0|\n",
      "| \"\"\" Bartleby\"|           1847|           1345|                   0.0|                   0.0|            589|                   0.0|\n",
      "|   \"\"\" Boomer\"|           3186|           1958|                   0.0|                   0.0|            144|                   0.0|\n",
      "+--------------+---------------+---------------+----------------------+----------------------+---------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "After Scaling :\n",
      "+--------------+---------------+---------------+----------------------+----------------------+---------------+----------------------+---------------+----------------------+\n",
      "|         ngram|occurrences2000|occurrences1990|occurrences1990_Scaled|occurrences2000_Scaled|occurrences1970|occurrences1970_Scaled|occurrences1980|occurrences1980_Scaled|\n",
      "+--------------+---------------+---------------+----------------------+----------------------+---------------+----------------------+---------------+----------------------+\n",
      "|         ! All|          95973|          33508|                 0.005|                  0.01|          14648|                 0.004|          19318|                 0.004|\n",
      "|    \"\"\" After\"|         645983|         254774|                 0.036|                 0.066|          78929|                 0.021|         123852|                 0.026|\n",
      "|\"\"\" Afterward\"|           6122|           3319|                   0.0|                 0.001|            836|                   0.0|           1709|                   0.0|\n",
      "| \"\"\" Bartleby\"|           1847|           1345|                   0.0|                   0.0|            589|                   0.0|           1519|                   0.0|\n",
      "|   \"\"\" Boomer\"|           3186|           1958|                   0.0|                   0.0|            144|                   0.0|            168|                   0.0|\n",
      "+--------------+---------------+---------------+----------------------+----------------------+---------------+----------------------+---------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "years = [1970,1980]\n",
    "for y in years:\n",
    "    address = \"s3://chenxiyuanly502/project/2gram_\"+str(y)\n",
    "    occur_col = \"occurrences\"+str(y)\n",
    "    \n",
    "    tmp_df = spark.read.csv(address)\n",
    "    \n",
    "    oldColumns = tmp_df.schema.names\n",
    "    newColumns = [\"ngram\", occur_col,\"books\",\"pages\"]\n",
    "    for i in range(len(oldColumns)):\n",
    "        tmp_df = tmp_df.withColumnRenamed(oldColumns[i], newColumns[i])\n",
    "    tmp_df = tmp_df.drop('books')\n",
    "    tmp_df = tmp_df.drop('pages')\n",
    "    \n",
    "    l1df = l1df.join(tmp_df, ['ngram'])\n",
    "    \n",
    "    l1df = l1df.withColumn(occur_col,  l1df[occur_col].cast('int'))\n",
    "    \n",
    "    i = occur_col\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "    # MinMaxScaler Transformation\n",
    "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "    # Pipeline of VectorAssembler and MinMaxScaler\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    # Fitting pipeline on dataframe\n",
    "    l1df = pipeline.fit(l1df).transform(l1df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "\n",
    "    print(\"After Scaling :\")\n",
    "    l1df.show(5)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919277"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1df = l1df.drop(\"occurrences2000\")\n",
    "l1df = l1df.drop(\"occurrences1990\")\n",
    "l1df = l1df.drop(\"occurrences1980\")\n",
    "l1df = l1df.drop(\"occurrences1970\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ngram: string (nullable = true)\n",
      " |-- occurrences1990_Scaled: double (nullable = true)\n",
      " |-- occurrences2000_Scaled: double (nullable = true)\n",
      " |-- occurrences1970_Scaled: double (nullable = true)\n",
      " |-- occurrences1980_Scaled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Scaling :\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ngram|occurrences1990_Scaled|occurrences2000_Scaled|occurrences1970_Scaled|occurrences1980_Scaled|occurrences1940_Scaled|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ! All|                 0.005|                  0.01|                 0.004|                 0.004|                 0.006|\n",
      "|    \"\"\" After\"|                 0.036|                 0.066|                 0.021|                 0.026|                 0.029|\n",
      "|\"\"\" Afterward\"|                   0.0|                 0.001|                   0.0|                   0.0|                   0.0|\n",
      "| \"\"\" Bartleby\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|   \"\"\" Boomer\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "After Scaling :\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ngram|occurrences1990_Scaled|occurrences2000_Scaled|occurrences1970_Scaled|occurrences1980_Scaled|occurrences1940_Scaled|occurrences1950_Scaled|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ! All|                 0.005|                  0.01|                 0.004|                 0.004|                 0.006|                 0.006|\n",
      "|    \"\"\" After\"|                 0.036|                 0.066|                 0.021|                 0.026|                 0.029|                 0.027|\n",
      "|\"\"\" Afterward\"|                   0.0|                 0.001|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "| \"\"\" Bartleby\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|   \"\"\" Boomer\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "After Scaling :\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ngram|occurrences1990_Scaled|occurrences2000_Scaled|occurrences1970_Scaled|occurrences1980_Scaled|occurrences1940_Scaled|occurrences1950_Scaled|occurrences1960_Scaled|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ! All|                 0.005|                  0.01|                 0.004|                 0.004|                 0.006|                 0.006|                 0.005|\n",
      "|    \"\"\" After\"|                 0.036|                 0.066|                 0.021|                 0.026|                 0.029|                 0.027|                 0.025|\n",
      "|\"\"\" Afterward\"|                   0.0|                 0.001|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "| \"\"\" Bartleby\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|   \"\"\" Boomer\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "years = [1940,1950,1960]\n",
    "for y in years:\n",
    "    address = \"s3://chenxiyuanly502/project/2gram_\"+str(y)\n",
    "    occur_col = \"occurrences\"+str(y)\n",
    "    \n",
    "    tmp_df = spark.read.csv(address)\n",
    "    \n",
    "    oldColumns = tmp_df.schema.names\n",
    "    newColumns = [\"ngram\", occur_col,\"books\",\"pages\"]\n",
    "    for i in range(len(oldColumns)):\n",
    "        tmp_df = tmp_df.withColumnRenamed(oldColumns[i], newColumns[i])\n",
    "    tmp_df = tmp_df.drop('books')\n",
    "    tmp_df = tmp_df.drop('pages')\n",
    "    \n",
    "    l1df = l1df.join(tmp_df, ['ngram'])\n",
    "    \n",
    "    l1df = l1df.withColumn(occur_col,  l1df[occur_col].cast('int'))\n",
    "    \n",
    "    i = occur_col\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "    # MinMaxScaler Transformation\n",
    "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "    # Pipeline of VectorAssembler and MinMaxScaler\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    # Fitting pipeline on dataframe\n",
    "    l1df = pipeline.fit(l1df).transform(l1df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "    \n",
    "    l1df = l1df.drop(occur_col)\n",
    "\n",
    "    print(\"After Scaling :\")\n",
    "    l1df.show(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ngram: string (nullable = true)\n",
      " |-- occurrences1990_Scaled: double (nullable = true)\n",
      " |-- occurrences2000_Scaled: double (nullable = true)\n",
      " |-- occurrences1970_Scaled: double (nullable = true)\n",
      " |-- occurrences1980_Scaled: double (nullable = true)\n",
      " |-- occurrences1940_Scaled: double (nullable = true)\n",
      " |-- occurrences1950_Scaled: double (nullable = true)\n",
      " |-- occurrences1960_Scaled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Scaling :\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ngram|occurrences1990_Scaled|occurrences2000_Scaled|occurrences1970_Scaled|occurrences1980_Scaled|occurrences1940_Scaled|occurrences1950_Scaled|occurrences1960_Scaled|occurrences1900_Scaled|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ! All|                 0.005|                  0.01|                 0.004|                 0.004|                 0.006|                 0.006|                 0.005|                 0.005|\n",
      "|    \"\"\" After\"|                 0.036|                 0.066|                 0.021|                 0.026|                 0.029|                 0.027|                 0.025|                 0.021|\n",
      "|\"\"\" Afterward\"|                   0.0|                 0.001|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|   \"\"\" Boomer\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|     \"\"\" Cade\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "After Scaling :\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ngram|occurrences1990_Scaled|occurrences2000_Scaled|occurrences1970_Scaled|occurrences1980_Scaled|occurrences1940_Scaled|occurrences1950_Scaled|occurrences1960_Scaled|occurrences1900_Scaled|occurrences1910_Scaled|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ! All|                 0.005|                  0.01|                 0.004|                 0.004|                 0.006|                 0.006|                 0.005|                 0.005|                 0.005|\n",
      "|    \"\"\" After\"|                 0.036|                 0.066|                 0.021|                 0.026|                 0.029|                 0.027|                 0.025|                 0.021|                 0.022|\n",
      "|\"\"\" Afterward\"|                   0.0|                 0.001|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|   \"\"\" Boomer\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|     \"\"\" Cade\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "After Scaling :\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ngram|occurrences1990_Scaled|occurrences2000_Scaled|occurrences1970_Scaled|occurrences1980_Scaled|occurrences1940_Scaled|occurrences1950_Scaled|occurrences1960_Scaled|occurrences1900_Scaled|occurrences1910_Scaled|occurrences1920_Scaled|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ! All|                 0.005|                  0.01|                 0.004|                 0.004|                 0.006|                 0.006|                 0.005|                 0.005|                 0.005|                 0.006|\n",
      "|    \"\"\" After\"|                 0.036|                 0.066|                 0.021|                 0.026|                 0.029|                 0.027|                 0.025|                 0.021|                 0.022|                 0.026|\n",
      "|\"\"\" Afterward\"|                   0.0|                 0.001|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|   \"\"\" Boomer\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|     \"\"\" Cade\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "After Scaling :\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ngram|occurrences1990_Scaled|occurrences2000_Scaled|occurrences1970_Scaled|occurrences1980_Scaled|occurrences1940_Scaled|occurrences1950_Scaled|occurrences1960_Scaled|occurrences1900_Scaled|occurrences1910_Scaled|occurrences1920_Scaled|occurrences1930_Scaled|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|         ! All|                 0.005|                  0.01|                 0.004|                 0.004|                 0.006|                 0.006|                 0.005|                 0.005|                 0.005|                 0.006|                 0.007|\n",
      "|    \"\"\" After\"|                 0.036|                 0.066|                 0.021|                 0.026|                 0.029|                 0.027|                 0.025|                 0.021|                 0.022|                 0.026|                 0.027|\n",
      "|\"\"\" Afterward\"|                   0.0|                 0.001|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|   \"\"\" Boomer\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "|     \"\"\" Cade\"|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|                   0.0|\n",
      "+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "years = [1900,1910,1920,1930]\n",
    "for y in years:\n",
    "    address = \"s3://chenxiyuanly502/project/2gram_\"+str(y)\n",
    "    occur_col = \"occurrences\"+str(y)\n",
    "    \n",
    "    tmp_df = spark.read.csv(address)\n",
    "    \n",
    "    oldColumns = tmp_df.schema.names\n",
    "    newColumns = [\"ngram\", occur_col,\"books\",\"pages\"]\n",
    "    for i in range(len(oldColumns)):\n",
    "        tmp_df = tmp_df.withColumnRenamed(oldColumns[i], newColumns[i])\n",
    "    tmp_df = tmp_df.drop('books')\n",
    "    tmp_df = tmp_df.drop('pages')\n",
    "    \n",
    "    l1df = l1df.join(tmp_df, ['ngram'])\n",
    "    \n",
    "    l1df = l1df.withColumn(occur_col,  l1df[occur_col].cast('int'))\n",
    "    \n",
    "    i = occur_col\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "    # MinMaxScaler Transformation\n",
    "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "    # Pipeline of VectorAssembler and MinMaxScaler\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    # Fitting pipeline on dataframe\n",
    "    l1df = pipeline.fit(l1df).transform(l1df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "    \n",
    "    l1df = l1df.drop(occur_col)\n",
    "\n",
    "    print(\"After Scaling :\")\n",
    "    l1df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ngram: string (nullable = true)\n",
      " |-- occurrences1990_Scaled: double (nullable = true)\n",
      " |-- occurrences2000_Scaled: double (nullable = true)\n",
      " |-- occurrences1970_Scaled: double (nullable = true)\n",
      " |-- occurrences1980_Scaled: double (nullable = true)\n",
      " |-- occurrences1940_Scaled: double (nullable = true)\n",
      " |-- occurrences1950_Scaled: double (nullable = true)\n",
      " |-- occurrences1960_Scaled: double (nullable = true)\n",
      " |-- occurrences1900_Scaled: double (nullable = true)\n",
      " |-- occurrences1910_Scaled: double (nullable = true)\n",
      " |-- occurrences1920_Scaled: double (nullable = true)\n",
      " |-- occurrences1930_Scaled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1df.write.csv(\"s3://chenxiyuanly502/project/scaler_00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866827"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_data = l1df.drop('ngram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=5, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=km_data.columns, outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(km_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmeans.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9826084853366345\n"
     ]
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[0.00064477 0.00097985 0.0005516  0.00062387 0.00057712 0.00057424\n",
      " 0.00061087 0.00043706 0.00050446 0.00055504 0.00054908]\n",
      "[0.45674766 0.69171963 0.38478505 0.4056729  0.43673832 0.43179439\n",
      " 0.44457944 0.3707757  0.4013271  0.43685981 0.43383178]\n",
      "[0.09912242 0.14967538 0.07727166 0.08450344 0.08510041 0.08429917\n",
      " 0.08707772 0.06880536 0.0751293  0.08216988 0.08285076]\n",
      "[0.24036559 0.37652957 0.1843414  0.20243011 0.20759946 0.20531183\n",
      " 0.21089785 0.17045699 0.1821586  0.20169892 0.20365323]\n",
      "[0.02688839 0.03959595 0.021109   0.02316722 0.02288174 0.02268258\n",
      " 0.02355062 0.01825093 0.02022091 0.02208472 0.02216752]\n"
     ]
    }
   ],
   "source": [
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
